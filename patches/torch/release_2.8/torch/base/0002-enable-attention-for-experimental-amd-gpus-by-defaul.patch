From 17f8e7ca6af7ca05994f38bd5c2023a6e87af771 Mon Sep 17 00:00:00 2001
From: Mika Laitio <lamikr@gmail.com>
Date: Wed, 15 Oct 2025 19:41:24 -0700
Subject: [PATCH] enable attention for experimental amd gpus by default

Signed-off-by: Mika Laitio <lamikr@gmail.com>
---
 .../src/ATen/native/transformers/cuda/sdp_utils.cpp | 14 ++++++--------
 1 file changed, 6 insertions(+), 8 deletions(-)

diff --git a/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp b/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp
index 2f815c3a..78ce774d 100644
--- a/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp
+++ b/aten/src/ATen/native/transformers/cuda/sdp_utils.cpp
@@ -288,9 +288,8 @@ bool check_flash_attention_hardware_support(sdp_params const& params, bool debug
     if (aotriton::isArchExperimentallySupported(stream)) {
       static const bool enable_experimental = c10::utils::check_env("TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL") == true;
       if (!enable_experimental) {
-        TORCH_WARN_ONCE("Flash Efficient attention on Current AMD GPU is still experimental."
-            " Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1.");
-        return false;
+        TORCH_WARN_ONCE("TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=0, Flash attention on current AMD GPU is still experimental.")
+        return true;
       }
     }
 #endif
@@ -320,7 +319,7 @@ bool check_mem_efficient_hardware_support(sdp_params const& params, bool debug)
   using sm50 = SMVersion<5, 0>;
   using sm121 = SMVersion<12, 1>;
 #if USE_ROCM
-#if USE_ROCM_ATTENTION
+  #if USE_ROCM_ATTENTION
   if(at::globalContext().getROCmFAPreferredBackend() == at::ROCmFABackend::Ck) {
     // User explicitly set CK as the flash attention backend. Return true for now
     // TODO: Flesh out sanity checks
@@ -331,7 +330,7 @@ bool check_mem_efficient_hardware_support(sdp_params const& params, bool debug)
         auto dprops = at::cuda::getCurrentDeviceProperties();
         if (debug) {
             TORCH_WARN(
-                    "Mem Efficient attention was not compiled for current AMD GPU architecture. Attempting to run on architecture ", dprops->gcnArchName);
+                    "Efficient attention was not compiled for current AMD GPU architecture. Attempting to run on architecture ", dprops->gcnArchName);
         }
         return false;
     }
@@ -339,9 +338,8 @@ bool check_mem_efficient_hardware_support(sdp_params const& params, bool debug)
     if (aotriton::isArchExperimentallySupported(stream)) {
       static const bool enable_experimental = c10::utils::check_env("TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL") == true;
       if (!enable_experimental) {
-        TORCH_WARN_ONCE("Mem Efficient attention on Current AMD GPU is still experimental."
-            " Enable it with TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1.");
-        return false;
+        TORCH_WARN_ONCE("TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=0, Efficient attention on current AMD GPU is still experimental.")
+        return true;
       }
     }
 #endif
-- 
2.43.0

